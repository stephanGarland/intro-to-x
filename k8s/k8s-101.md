# Introduction

## What is Kubernetes?

Kubernetes is a container orchestration system. It manages the scheduling and execution of containers. Similar platforms exist like Docker Swarm, Apache Mesos, and Hashicorp Nomad. Kubernetes by far has the dominant market share, and is also the most complex out of those listed.

## Kubernetes components

### High level
* Cluster: a logical grouping of one or more nodes.
* Node: a server running Kubernetes - can be bare metal, a VM, or a container.
* Pod: a logical grouping of one or more containers.
* Container: the same as Docker.

#### Node roles
* Control plane: schedules pods, detects and responds to cluster events, maintains cluster state in a database.
* Worker: runs user-defined workloads via DaemonSets, StatefulSets, or Deployments.
	* Note that while not recommended in production, for development purposes, a single-node cluster can serve as both of these roles.

### Low[er] level

For a more thorough examination of Kubernetes components, [the official documentation](https://kubernetes.io/docs/concepts/overview/components/) is recommended. A brief overview of some components follows:

* kube-apiserver: handles requests to the API, typically via kubectl.
* etcd: a key/value store utilizing the Raft algorithm for consensus; frequently used as the store for Kubernetes cluster data.
	* K3s (and thus K3d) uses an embedded SQLite database as its backing store by default; in general any database may be used, but in production etcd is the standard.
* kube-scheduler: assigns workloads to a node, constrained by resource limits, affinity/anti-affinity rules, etc.
* kubelet: an agent running on every node, ensuring that a Pod's containers are running.

## Kubernetes distributions

Each cloud provider has their own - Amazon has EKS, Azure has AKS, Google has GKE, DigitalOcean has DOKS, etc. Vanilla Kubernetes can be installed [either manually](https://github.com/kelseyhightower/kubernetes-the-hard-way), or with a tool like [kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/). Various distributions also exist, much like Linux distributions. [Minikube](https://minikube.sigs.k8s.io/docs/) is a popular way to bootstrap a single-node cluster for development in an existing operating system. K3d is based on [k3s](https://k3s.io/), which is a lightweight single-binary distribution of Kubernetes. Rancher Labs (owned by SuSE) also make a full single-purpose OS called [k3os](https://k3os.io/) which is designed to run k3s, and only k3s. A similar (albeit running vanilla Kubernetes) but even more extreme example is [Talos](https://www.talos.dev/), which is completely immutable, has no shell access, and allows access only via its API. Amazon has a similar offering called [Bottlerocket](https://aws.amazon.com/bottlerocket/).

In general, any Kubernetes distribution will be perfectly adequate for learning, and it comes down to personal preference. For production, there are arguments to be made for managed services like EKS, but that's beyond the scope of this document.

# Getting started

## Install

### Prerequisites:

 - Docker
	 - There are many ways to do this, pick your favorite
 - kubectl
	 -	`brew install kubectl`
	 -	Optional:
		 -	`brew install kubecolor`
		 -	Add the following to your shell rc file:
			 -	`compdef kubecolor=kubectl`
			 -	`alias kubectl=kubecolor`
       -  `alias k=kubectl` OR install your shell's plugin for kubectl

### Install and verification
First, run `brew install minikube`. Then, run `minikube start` with a few options: `‚ùØ minikube start --memory 8GB --cpus 4 --driver hyperkit`. Assuming you have the memory and CPU to spare, this ensures we won't run into any backing hardware issues. `hyperkit` as the driver means we don't have to download anything additional to spin up the VM that runs the cluster. 

    ‚ùØ minikube start --memory 8GB --cpus 4 --driver hyperkit
    üòÑ  minikube v1.25.2 on Darwin 11.6.5
        ‚ñ™ KUBECONFIG=/Users/sgarland/.kube/.switch_tmp/config.1541775917.tmp
        ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube
    ‚ú®  Using the hyperkit driver based on user configuration
    üëç  Starting control plane node minikube in cluster minikube
    üî•  Creating hyperkit VM (CPUs=4, Memory=8192MB, Disk=20000MB) ...
    üê≥  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
        ‚ñ™ kubelet.housekeeping-interval=5m
        ‚ñ™ Generating certificates and keys ...
        ‚ñ™ Booting up control plane ...
        ‚ñ™ Configuring RBAC rules ...
    üîé  Verifying Kubernetes components...
        ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
    üåü  Enabled addons: storage-provisioner, default-storageclass
    üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

Next, we'll enable the registry addon: 

    ‚ùØ minikube addons enable registry
        ‚ñ™ Using image registry:2.7.1
        ‚ñ™ Using image gcr.io/google_containers/kube-registry-proxy:0.4
    üîé  Verifying registry addon...
    üåü  The 'registry' addon is enabled

Next, if you don't already have the docker daemon, we'll hook into Minikube's: `eval $(minikube -p minikube docker-env)`

Finally, we need to modify networking a little bit using `socat` to get the registry to listen to our local docker daemon:

    ‚ùØ docker run --rm -it --network=host alpine ash -c "apk add socat && socat TCP-LISTEN:5000,reuseaddr,fork TCP:$(minikube ip):5000"
    Unable to find image 'alpine:latest' locally
    latest: Pulling from library/alpine
    df9b9388f04a: Already exists
    Digest: sha256:4edbd2beb5f78b1014028f4fbb99f3237d9561100b6881aabbf5acce2c4f9454
    Status: Downloaded newer image for alpine:latest
    fetch https://dl-cdn.alpinelinux.org/alpine/v3.15/main/x86_64/APKINDEX.tar.gz
    fetch https://dl-cdn.alpinelinux.org/alpine/v3.15/community/x86_64/APKINDEX.tar.gz
    (1/4) Installing ncurses-terminfo-base (6.3_p20211120-r0)
    (2/4) Installing ncurses-libs (6.3_p20211120-r0)
    (3/4) Installing readline (8.1.1-r0)
    (4/4) Installing socat (1.7.4.2-r0)
    Executing busybox-1.34.1-r5.trigger
    OK: 7 MiB in 18 packages

Let's verify the cluster:

	‚ùØ kubectl get nodes
    NAME       STATUS   ROLES                  AGE     VERSION
    minikube   Ready    control-plane,master   6m26s   v1.23.3

For more detail, use `describe`. There's a lot here, but I'll highlight some pertinent information.

	‚ùØ kubectl describe nodes
	Name:               minikube
	Roles:              control-plane,master
	Labels:             beta.kubernetes.io/arch=amd64
	...
	Capacity:
    cpu:                4
    ephemeral-storage:  17784752Ki
    hugepages-2Mi:      0
    memory:             8161900Ki
    pods:               110
	...
	Events:
	  Type    Reason                   Age                    From        Message
    ----    ------                   ----                   ----        -------
    Normal  Starting                 6m28s                  kube-proxy
    Normal  NodeHasSufficientMemory  6m52s (x5 over 6m52s)  kubelet     Node minikube status is now: NodeHasSufficientMemory
    Normal  NodeHasNoDiskPressure    6m52s (x5 over 6m52s)  kubelet     Node minikube status is now: NodeHasNoDiskPressure
    Normal  NodeHasSufficientPID     6m52s (x4 over 6m52s)  kubelet     Node minikube status is now: NodeHasSufficientPID
    Normal  Starting                 6m42s                  kubelet     Starting kubelet.
    Normal  NodeHasNoDiskPressure    6m42s                  kubelet     Node minikube status is now:   NodeHasNoDiskPressure
    Normal  NodeHasSufficientPID     6m42s                  kubelet     Node minikube status is now: NodeHasSufficientPID
    Normal  NodeNotReady             6m42s                  kubelet     Node minikube status is now: NodeNotReady
    Normal  NodeAllocatableEnforced  6m42s                  kubelet     Updated Node Allocatable limit across pods
    Normal  NodeHasSufficientMemory  6m42s                  kubelet     Node minikube status is now: NodeHasSufficientMemory
    Normal  NodeReady                6m31s                  kubelet     Node minikube status is now: NodeReady
  
At the top, we can see the name, role, labels, and annotations. The name is self-explanatory. The role here is showing two - control-plane, and master. These are the same thing, and are in parallel since Kubernetes v1.20. `master` is being deprecated in favor of `control-plane` and will be fully removed in a future release. The purpose and limitations of this, along with taints, will be discussed later. Labels are key/value pairs that can be arbitrarily applied, but usually carry semantic meaning for either the user or an application. For example, `kubernetes.io/arch=amd64` tells us that this node has `amd64` architecture. Clusters can be of mixed architecture, so it's good to be able to easily tell apart `x86` and `arm` nodes for scheduling purposes.

Let's label the node, for fun:

	  ‚ùØ kubectl label node --all "my.name.is=$(whoami)"
	  node/minikube labeled
Using the `--all` flag applies it to all nodes; without it, you'd need to add the node's name (`k3d-sgarland-cluster-server-0` for me).

We can then see the new label with the `--show-labels `flag:

	  ‚ùØ kubectl get nodes --show-labels
	  NAME       STATUS   ROLES                  AGE     VERSION   LABELS
    minikube   Ready    control-plane,master   9m32s   v1.23.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7,minikube.k8s.io/name=minikube,minikube.k8s.io/primary=true,minikube.k8s.io/updated_at=2022_05_06T14_14_05_0700,minikube.k8s.io/version=v1.25.2,my.name.is=sgarland,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=

Unfortunately it's a bit messy in the default comma-separated form, but should be able to spot your change in there. To delete the label, the syntax is somewhat confusing; you use the key and a `-` sign to indicate that it should be removed:

    ‚ùØ kubectl label node --all "my.name.is-"
    node/minikube labeled

Next is Capacity. We can see things like the amount of ephemeral storage and memory available (16 GB and 8 GB, respectively) to the cluster, as well as allocatable pods. 110 pods is not actually resource-based, but networking - with a `/24` block being assigned to each node, there are 256 addresses available. With slightly over double the amount of addresses than the maximum number of pods, this reduces IP address reuse as pods come and go from the node.

Finally, Events. In this section, the kubelet reports the status of the node, here showing that it has sufficient memory, disk, PID, and is ready.

# Exploration

## Imperative vs Declarative

Ideally, everything is maintained in code, and changes are made with some form of state management, be it ArgoCD, Flux, or others. Less optimally, you can issue commands with `kubectl apply`, which reads your input file and compares it to existing, then makes changes. Even less optimally, you can directly issue `kubectl` commands.

## Kubectl verbs

So far we've used a few - `get`, `describe`, and `label`. Kubernetes [loosely follows HTTP verbs](https://kubernetes.io/docs/reference/access-authn-authz/authorization/#determine-the-request-verb), with some extras thrown in. One important note is that since you are directly communicating with the API, there are no warnings for destructive actions. If you tell it to delete a Persistent Volume, it will do so (with some exceptions for finalizers).

## Create a deployment

Let's deploy a simple application. If you have a small Dockerized app you'd like to run you're welcome to use it here, but otherwise, we'll focus on this simple echo app that echos the user's input.

### Building the application
Use your own, or copy/paste this into a shell to write them to `echo.py` and `Dockerfile`, respectively.

    cat << EOF > echo.py
    #!/usr/bin/env python

	def main():
	    while True:
	        user_input = input("Hi, say something, or type 'quit' to quit: ")
	        if user_input == "quit":
	            break
	        else:
	            print(user_input)

	if __name__ == "__main__":
	    main()
	
	EOF


---
    cat << EOF > Dockerfile
	FROM python:3.10-alpine
	
	WORKDIR /app
	
	COPY ./echo.py /app/echo.py
	
	CMD ["python", "/app/echo.py"]
	
	EOF

Then, build it with `docker build -t echo .` (assuming you placed both files in your current working directory).

To test that it works, you can use `docker run --rm -i --name echo echo`.  Bonus question: what does the `-i` flag do, and what happens if you neglect to include it here?

### Writing a Deployment
A Deployment is a basic Kubernetes structure, which allows you to define a workload in the form of a Pod, which should run with n replicas. Via the kubelet, a Deployment can ensure that an app is restarted if it fails, is reachable (assuming you've set up liveness and readiness probes), and more.

#### YAML
    cat << EOF > deployment.yaml
    apiVersion: apps/v1
	kind: Deployment
	metadata:
	  name: echo
	spec:
	  selector:
	    matchLabels:
	      app: echo
	  replicas: 1
	  template:
	    metadata:
	      labels:
	        app: echo
	    spec:
	      containers:
	      - name: echo
	        image: localhost:5000/echo:latest
	        imagePullPolicy: Always
	        stdin: true
	        tty: true
	
	EOF

Let's break down what's going on here, line by excruciating line:

    # This refers to a specific API version for the code
    # that follows - these are regularly updated and
    # deprecated, but you're warned well in advance
    apiVersion: apps/v1
    
    # This specifies what it is you're defining - could
    # also be a StatefulSet, an Ingress, a Service, etc.
    kind: Deployment
    
    # You can put multiple things here; the two most
    # common are the name of the application, and
    # a namespace in which to install it
    metadata:
      name: echo
    
    # This tells the Deployment what application
    # it should manage - in this case, it's looking
    # for those with the label `app: echo`
    spec:
      selector:
        matchlabels:
          app: echo
      # The number of replicas to deploy - note that
      # this is even with `spec.selector`, and like Python,
      # whitespace is extremely important
      replicas: 1

      # This gives the Pods a template to apply
      # In this case, the label `app: echo`
      template:
        metadata:
          labels:
            app: echo
        # Now we define the Pod's containers - note that this
        # is even with `template.metadata`, as it is part
        # of the template
        spec:
          containers:
          # The name of your application
          - name: echo
            # The image, optionally as a FQDN
            # If not specified as a FQDN, it will first 
            # be searched for locally, and then on Dockerhub
            image: localhost:5000/echo:latest
            # When to pull - can also use Never or IfNotPresent
            imagePullPolicy: IfNotPresent
            # Technically only stdin is needed, but
            # if you don't also give it a psuedo-TTY
            # it will complain (but still run) when
            # you attach to the container
            stdin: true
            tty: true

### Applying the Deployment

#### Pushing the build
But first, we have to tag and push to our registry.

    ‚ùØ docker tag echo:latest localhost:5000/echo:latest
    ‚ùØ docker push localhost:5000/echo:latest
	  The push refers to repository [localhost:5000/echo]
	  43358167f05b: Layer already exists
    96568c21d3ac: Layer already exists
    b02dd59d34c0: Layer already exists
    0b800261971d: Layer already exists
    16e3ab2d4dee: Layer already exists
    fbd7d5451c69: Layer already exists
    4fc242d58285: Layer already exists
    latest: digest: sha256:36450f0ec0febf8daf800f24ab81363211dc52dd6bfc3e50d5d54c508f8d89ed size: 1782

#### Deploy!
As stated, there are far better ways to deploy applications, but this is the most basic, and gives the most insight into what Kubernetes is doing to get your app running.

If you run all of these in quick succession, you should see the following:

    ‚ùØ kubectl apply -f deployment.yaml
    deployment.apps/echo created

    ‚ùØ kubectl get deployments
    NAME   READY   UP-TO-DATE   AVAILABLE   AGE
    echo   0/1     1            0           1s

    ‚ùØ kubectl get pods
    NAME                    READY   STATUS              RESTARTS   AGE
    echo-746cdbd89c-hrzds   0/1     ContainerCreating   0          2s

Once the pod creates and deploys (which for this, takes a very short amount of time), the latter two commands should show this:

    ‚ùØ kubectl get deployments
    NAME   READY   UP-TO-DATE   AVAILABLE   AGE
    echo   1/1     1            1           2m6s
    
    ‚ùØ kubectl get pods
    NAME                    READY   STATUS    RESTARTS   AGE
    echo-746cdbd89c-hrzds   1/1     Running   0          2m35s

### Exploring the Deployment
Let's apply some of the verbs available to us.

#### Attach
    ‚ùØ kubectl attach -i echo-746cdbd89c-hrzds
    If you don't see a command prompt, try pressing enter.


    Hi, say something, or type 'quit' to quit: Hello!
    Hello!
    Hi, say something, or type 'quit' to quit: quit
    Session ended, resume using 'kubectl attach echo-746cdbd89c-hrzds -c echo -i -t' command when the pod is running

`attach` lets us attach to a container's default process, which in this case, is our app.

#### Exec

You could also use `exec` to get a shell into the pod, like this:

    ‚ùØ kubectl exec -it echo-74bf7cdf5c-9rhxd -- sh
    Error from server (NotFound): pods "echo-74bf7cdf5c-9rhxd" not found

#### Describe
What's this? Our pod went away already? Let's `describe` the new pod to see why.

    ‚ùØ kubectl describe pod echo-746cdbd89c-hrzds
    Name:         echo-746cdbd89c-hrzds
    Namespace:    default
    ... (not shown for conciseness)
    Containers:
      echo:
        ...
         Last State:     Terminated
         Reason:       Completed
         Exit Code:    0
         Started:      Fri, 25 Mar 2022 15:03:57 -0500
         Finished:     Fri, 25 Mar 2022 15:05:24 -0500

Ah, there we are - since our program runs in a loop until it receives `quit` as input, once that was passed, the program exited. The kubelet noticed that the deployment no longer had a running pod, and spawned a new one.

#### Get

We can see this if we `get` pods:

    ‚ùØ kubectl get pods
    NAME                    READY   STATUS    RESTARTS      AGE
    echo-746cdbd89c-hrzds   1/1     Running   1 (62s ago)   8m

#### Exec (again)
Now let's exec into the pod.

    ‚ùØ kubectl exec -it echo-746cdbd89c-hrzds -- sh
    /app # ls
    echo.py
    /app # python echo.py
    Hi, say something, or type 'quit' to quit: Hello
    Hello
    Hi, say something, or type 'quit' to quit: quit
    /app #
Note that here, quitting the app didn't kill the pod - that's because we spawned a new shell to exec into, and created a new instance of the app. Look at what's running:

    /app # ps
    PID   USER     TIME  COMMAND
    1 root      0:00 python /app/echo.py
    27 root      0:00 sh
    41 root      0:00 ps

Our app is running as the `init` process, PID 1. Kill it and watch what happens. Just kidding - `init` traps most `kill` signals in reasonable *nix distributions for good reason; but you can send it `INT` aka `2` if you'd like to see what happens (you could also kill the shell, if you'd like).

#### Delete

This is how you canonically restart a pod, in case you weren't aware.

    ‚ùØ kubectl delete pod -l app=echo
    pod "echo-746cdbd89c-hrzds" deleted
  
What's this `-l` flag? Why didn't we have to specify the entire name? Welcome to selectors - also available with their longhand flag, `--selector`. Remember the `template.metadata.labels.app` we assigned to the Deployment? That's how this is finding it.
And we can see that we now have a new pod, thanks to the Deployment: 

    ‚ùØ kubectl get pods
    NAME                    READY   STATUS    RESTARTS   AGE
    echo-746cdbd89c-x9k2m   1/1     Running   0          36s


### Scaling workloads

If you have a given workload, be it a Deployment or StatefulSet, you can horizontally scale it using the command `kubectl scale`, and the flag `--replicas`. Go ahead and scale ours up to, say, 3 replicas:

    ‚ùØ kubectl scale deployment echo --replicas=3
    deployment.apps/echo scaled

Now let's look at our deployment (if you aren't quick, you might just see 3/3 ready, but that's OK):

    ‚ùØ kubectl get deployments
    NAME   READY   UP-TO-DATE   AVAILABLE   AGE
    echo   1/3     3            1           68m
 
Once the pods are all up, this will change to 3/3 ready.

    ‚ùØ kubectl get pods
    NAME                    READY   STATUS    RESTARTS   AGE
    echo-746cdbd89c-8v5qb   1/1     Running   0          3s
    echo-746cdbd89c-ns9kk   1/1     Running   0          3s
    echo-746cdbd89c-x9k2m   1/1     Running   0          26m

Of note, all this time we haven't been specifying a deployment (or pod) for `get`, which is fine since we're only running the one. If this were a real cluster, though, there would likely be many deployments and pods, and we'd want to be more specific:

    ‚ùØ kubectl get deployment echo
    NAME   READY   UP-TO-DATE   AVAILABLE   AGE
    echo   3/3     3            3           71m

### Scaling (down) workloads

To horizontally scale to zero, AKA delete the pods and prevent them from coming back, use `--replicas` again, but specify 0 pods: `--replicas=0`. Alternately, if you want to completely get rid of the deployment, use either `kubectl delete deployment/echo` (imperative) or `kubectl delete -f deployment.yaml` (declarative). With the latter, kubectl is reading the deployment manifest we wrote, and removing it.

Either way, once done, we can verify that it's gone:

    ‚ùØ kubectl get deployment
    No resources found in default namespace.

## Namespaces

We've briefly mentioned namespaces so far, but all the work has been done in the default namespace. This is generally a bad idea - namespaces are a way of organizing and restricting resources. We can limit a given namespace to X CPUs and Y memory, we can restrict the rights of workloads inside that namespace, and it makes it easier to keep track of things when running various `kubectl` commands if it's scoped to a namespace.

Let's create one imperatively and use it, and then create another declaratively.

    ‚ùØ kubectl create namespace echo
    namespace/echo created

Now let's deploy our echo app in the new namespace:

    ‚ùØ kubectl apply -f deployment.yaml -n echo
    deployment.apps/echo created

Note that the pod isn't in the default namespace anymore:

    ‚ùØ kubectl get pods
    No resources found in default namespace.

    ‚ùØ kubectl get pods -n echo
    NAME                   READY   STATUS    RESTARTS   AGE
    echo-d97d96459-s2bvk   1/1     Running   0          79s

Now, let's delete it and then do it again declaratively (delete the deployment however you'd like, as described earlier).

    ‚ùØ sed -i'' '/^spec:/i \ \ namespace: echo' deployment.yaml

This adds a properly spaced `.metadata.namespace` line to the deployment manifest, looking for the target `^spec:` line and then going immediately before that.

    ‚ùØ kubectl get pods -n echo
    NAME                   READY   STATUS    RESTARTS   AGE
    echo-d97d96459-9l2jw   1/1     Running   0          4s

There's our pod! What if we wanted to declaratively create the namespace, as well? Let's delete the namespace, which will also delete the deployment (not recommended in prod due to finalizers, but for this example it's fine):

    ‚ùØ kubectl delete namespace echo
    namespace/echo deleted

  ---

    ex deployment.yaml <<EOF
    1 insert
    apiVersion: v1
    kind: Namespace
    metadata:
      name: echo
    ---
    .
    xit
    EOF

You can simply insert everything from `apiVersion` to `EOF` into the file if you'd like, but this is a mildly interesting way to insert arbitrary text into the beginning of a file without [explicitly] using a temporary file. It has to be inserted at the beginning because otherwise when we apply it, the deployment will fail since the namespace doesn't exist yet (but it would succeed with a second `apply` command). If you think this is annoying, you're right, and there are ways around it.

Applying the deployment now creates both the namespace, and deploys to it:

    ‚ùØ kubectl apply -f deployment.yaml
    namespace/echo created
    deployment.apps/echo created

## Helm

This is all terrifically annoying, though - no one wants to be issuing `kubectl` commands directly against clusters. Luckily, there's a better way - [Helm](https://helm.sh/).

### Installation

`brew install helm`

### Components

I'll defer to Helm's excellent docs, but in short, a Helm chart consists at its core of a simple file structure:

    ‚ùØ tree
    .
    ‚îú‚îÄ‚îÄ Chart.yaml
    ‚îî‚îÄ‚îÄ templates
        ‚îú‚îÄ‚îÄ NOTES.txt
        ‚îú‚îÄ‚îÄ deployment.yaml
        ‚îú‚îÄ‚îÄ ingress.yaml
        ‚îú‚îÄ‚îÄ namespace.yaml
        ‚îî‚îÄ‚îÄ service.yaml

    1 directory, 6 files

`Chart.yaml` is required, but doesn't have to contain a lot:

    ‚ùØ mkdir helm

    cat << EOF > helm/Chart.yaml
    apiVersion: v2
    name: echo
    description: A Helm chart for a simple echo app
    version: 0.1.0
    appVersion: 0.1.0
    EOF

`version` is the version of the Helm chart, whereas `appVersion` is the version of the application. They should both use semantic versioning. `apiVersion` would be `v1` if you needed Helm v2 compatibility, but no one should be using Helm v2 these days, so stick with `apiVersion: v2`.

    ‚ùØ mkdir templates

    cat << EOF > helm/templates/deployment.yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: echo
      namespace: echo
    spec:
      selector:
        matchLabels:
          app: echo
      replicas: 1
      template:
        metadata:
          labels:
            app: echo
        spec:
          containers:
          - name: echo
            image: localhost:5000/echo:latest
            imagePullPolicy: Always
            ports:
              - containerPort: 8080
            stdin: true
            tty: true
    EOF

The eagle-eyed among you will note that this is largely the same, except that we've added a `containerPort` that we'll be talking to.

    cat << EOF > helm/templates/service.yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: echo
      namespace: echo
    spec:
      ports:
        - protocol: TCP
          port: 8080
          targetPort: 8080
      selector:
        app: echo
      type: NodePort
    EOF

The Service will connect our app out to the cluster - in this case, via a NodePort, which means that a random port will be opened on every node. `targetPort` is actually redundant here, as it defaults to the same port as `port`, but it's shown for education. In production, you would typically use a `LoadBalancer` service.

    cat << EOF > helm/templates/ingress.yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: echo
      namespace: echo
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /$1
    spec:
      rules:
      - host: echo.internal
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: echo
                port:
                  number: 8080
    EOF

We're using an Ingress here to route traffic to the service, and ultimately, to the pod.

    cat << EOF > helm/templates/namespace.yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      name: echo
    EOF

The Namespace definition hasn't changed. We could also rely on Helm to do this for us, with its `--create-namespace` flag.

    cat << EOF > helm/templates/NOTES.txt
    To access, please run the following command:

        sudo echo "$(minikube ip) echo.internal" >> /etc/hosts

    Then go to http://echo.internal inyour browser.

    To clean up, run the following command:

        sudo sed -i'' '$d' /etc/hosts
    EOF

`NOTES.txt` is a special file for Helm, which it will render when you run `helm install` as helpful tips to the user. In this case, we're explaining how to edit the `/etc/hosts` file so that the URI resolves.

### App

But wait, I hear you saying, the app didn't have any web server! You're correct, so let's remedy that quickly:

    ‚ùØ mkdir -p echo/templates

    cat << EOF > echo/echo.py
    #!/usr/bin/env python

    from flask import Flask, request, render_template

    app = Flask(__name__)

    @app.route("/")
    def form():
        return render_template("index.html")

    @app.route("/", methods=["POST"])
    def form_post():
        return request.form["echo_input"]

    if __name__ == "__main__":
        app.run(host="0.0.0.0", port=8080, debug=True)
    EOF

  ---

    cat << EOF > echo/templates/index.html
    <html>
    <head>
      <title>Echo (echo...)</title>
    </head>
    <body>
        <h1>Echo (echo...)</h1>
        <form method="POST">
            <input type="text" name="echo_input" placeholder="Say something!">
            <input type="submit" value="Echo!">
        </form>
    </body>
    </html>
    EOF

(No one will ever accuse me of being a frontend dev. I regret nothing.)

We need to make sure Docker can install Flask (ideally this would be pinned to a specific version): `echo "flask" > echo/requirements.txt`

Finally, we need to update the `Dockerfile`.

    cat << EOF > echo/Dockerfile
    FROM python:3.10-alpine

    WORKDIR /app

    COPY . /app

    RUN pip install -r requirements.txt

    EXPOSE 8080

    CMD ["python", "/app/echo.py"]
    EOF

### Installation

To install the Chart, let's first see what it would do:

    ‚ùØ helm install --dry-run --debug echo helm/
    install.go:178: [debug] Original chart version: ""
    install.go:195: [debug] CHART PATH: /Users/sgarland/git/zapier/intro-to-x/k8s/helm

    NAME: echo
    LAST DEPLOYED: Fri May  6 17:02:50 2022
    NAMESPACE: default
    STATUS: pending-install
    REVISION: 1
    TEST SUITE: None
    USER-SUPPLIED VALUES:
    {}

    COMPUTED VALUES:
    {}

    HOOKS:
    MANIFEST:
    ---
    # Source: echo/templates/namespace.yaml
    apiVersion: v1
    kind: Namespace
    metadata:
      name: echo
    ---
    # Source: echo/templates/service.yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: echo
      namespace: echo
    spec:
      ports:
        - protocol: TCP
          port: 8080
          targetPort: 8080
      selector:
        app: echo
      type: NodePort
    ---
    # Source: echo/templates/deployment.yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: echo
      namespace: echo
    spec:
      selector:
        matchLabels:
          app: echo
      replicas: 1
      template:
        metadata:
          labels:
            app: echo
        spec:
          containers:
          - name: echo
            image: localhost:5000/echo:latest
            imagePullPolicy: Always
            ports:
              - containerPort: 8080
            stdin: true
            tty: true
    ---
    # Source: echo/templates/ingress.yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: echo
      namespace: echo
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
    spec:
      rules:
      - host: echo.internal
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: echo
                port:
                  number: 8080

    NOTES:
    To access, please run the following command:

        sudo echo "$(minikube ip) echo.internal" >> /etc/hosts

    Then go to http://echo.internal inyour browser.

    To clean up, run the following command:

        sudo sed -i'' '$d' /etc/hosts

Looks good! To install it, we can use the `ugprade` command with the `--install` flag - this way, if we need to make any changes, we don't have to type out a new command.

    ‚ùØ helm upgrade --install echo helm
    Release "echo" does not exist. Installing it now.
    NAME: echo
    LAST DEPLOYED: Fri May  6 17:04:43 2022
    NAMESPACE: default
    STATUS: deployed
    REVISION: 1
    TEST SUITE: None
    NOTES:
    To access, please run the following command:

        sudo echo "$(minikube ip) echo.internal" >> /etc/hosts

    Then go to http://echo.internal inyour browser.

    To clean up, run the following command:

        sudo sed -i'' '$d' /etc/hosts

Let's add the `/etc/hosts` entry, then we can test it out!

    ‚ùØ sudo echo "$(minikube ip) echo.internal" >> /etc/hosts
    ‚ùØ curl -d 'echo_input=Hello, world!' -X POST http://echo.internal
    Hello, world!

# RBAC

RBAC is Role-based Access Control. It's a way to control access to resources based on a user (or group's) role, rather than their identity. The assumption is that you have something else (like Okta) to authenticate the user, and then, RBAC will control that user's ability to access or modify resources.

## Example

### Generating a certificate

We're going to create a certificate and user to demonstrate how RBAC works.

    ‚ùØ openssl genrsa -out echo-user.key 4096 && openssl req -new -key echo-user.key -out echo-user.csr -subj "/CN=echo-user/O=echo-group" && openssl x509 -req -in echo-user.csr -CA ~/.minikube/ca.crt -CAkey ~/.minikube/ca.key -CAcreateserial -out echo-user.crt -days 365 || echo "Failed to create cert! Please check that ~/.minikube/ca.{crt,key} exist."
    Generating RSA private key, 4096 bit long modulus
    ......................................................................................................................................................................................++
    ......................................................................................................................++
    e is 65537 (0x10001)
    Signature ok
    subject=/CN=echo-user/O=echo-group
    Getting CA Private Key

This one-liner uses the `openssl` tool to first create a 4096-bit RSA private key, then requests a Certificate Request using that key, and finally creates a certificate signed by the Minikube Certificate Authority, with an expiry of 365 days. The ending part, if you're not familiar with shell, is an `OR` that only executes if the previous command fails - since that command is relying on two files existing in `~/.minikube`, there's a pretty good chance that they're the reason for the failure, hence the message.

### Creating a user

Now, we're going to create a user entry in our kubeconfig, then create a context using it.

    ‚ùØ kubectl config set-credentials echo-user --client-certificate=echo-user.crt --client-key=echo-user.key
    User "echo-user" set.

    ‚ùØ kubectl config set-context echo-user-context --cluster=minikube --user=echo-user
    Context "echo-user-context" created.

    ‚ùØ kubectl config use-context echo-user-context
    Switched to context "echo-user-context".

### Testing out the user

Let's create a namespace again:

    ‚ùØ kubectl create ns foobar
    Error from server (Forbidden): namespaces is forbidden: User "echo-user" cannot create resource "namespaces" in API group "" at the cluster scope

Since Minikube is installed with its default context of `minikube`, this additional user we've added has no permissions to do, well, anything. Try `kubectl get pods` or some other read-only action, and check the result.

### Adding RBAC

RBAC definitions consist of two parts - Role, and RoleBinding - and are are either scoped to a cluster, or to a namespace. Helpfully, cluster-scoped RBAC objects are named ClusterRoles and ClusterRoleBindings.

    cat << EOF > helm/templates/rbac.yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: echo-ro
      namespace: echo
    rules:
      - apiGroups: [""]
        resources: ["pods"]
        verbs: ["get", "list", "watch"]
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: echo-ro
      namespace: echo
    subjects:
    - kind: User
      name: echo-user
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: Role
      name: echo-ro
      apiGroup: rbac.authorization.k8s.io
    EOF

This is two RBAC objects in one file - a Role, and a RoleBinding. They're both scoped to the `echo` namespace, and as the name implies, they create a read-only role for the `echo-user` user we previously created. Note that you'll need to switch back to the `minikube` context to apply this (do you remember how?).

    ‚ùØ helm upgrade --install echo helm
    Release "echo" has been upgraded. Happy Helming!
    NAME: echo
    LAST DEPLOYED: Wed May 18 10:51:41 2022
    NAMESPACE: default
    STATUS: deployed
    REVISION: 2
    TEST SUITE: None
    NOTES:
    To access, please run the following command:

        sudo echo "$(minikube ip) echo.internal" >> /etc/hosts

    Then go to http://echo.internal inyour browser.

    To clean up, run the following command:

        sudo sed -i'' '$d' /etc/hosts

### Verifying RBAC

We can of course use `kubectl get role -n echo` and `kubectl get rolebinding -n echo` to view our newly-available RBAC, but `kubectl` includes a very useful feature called `kubectl auth can-i` which allows you to check if you have the ability to do a given action, as a given user and/or group. Cluster administrators can impersonate another user (this is very useful for SREs) with the `--as user.name` flag, but anyone can use it to check their current ability.

    ‚ùØ kubectl config use-context echo-user-context
    Switched to context "echo-user-context".

    ‚ùØ kubectl auth can-i get pods -n echo
    yes

    ‚ùØ kubectl auth can-i get pods -n echo --as foobar
    Error from server (Forbidden): users "foobar" is forbidden: User "echo-user" cannot impersonate resource "users" in API group "" at the cluster scope

    ‚ùØ kubectl auth can-i create pods -n echo
    no

    ‚ùØ kubectl auth can-i get pods -n kube-system
    no

    ‚ùØ kubectl auth can-i create pods --subresource exec -n echo
    no

This last one can be problematic, and indeed, is/was the source of much pain in SRE land as developers were unable to exec into bastion pods. `exec` is a subset of `create`, and specific permission must be granted to do so. If you try without having the requisite permission, you'll see this:

    ‚ùØ kubectl exec -it -n echo echo-75897c68fd-nhn64 -- sh
    Error from server (Forbidden): pods "echo-75897c68fd-nhn64" is forbidden: User "echo-user" cannot create resource "pods/exec" in API group "" in the namespace "echo"

### Modifying RBAC

    ‚ùØ ex helm/templates/rbac.yaml <<EOF
      ?---? insert
        - apiGroups: [""]
          resources: ["pods/exec"]
          verbs: ["create"]
      .
      :%s/echo-ro/echo-rw/g
      xit
      EOF

Or, you know, use an actual editor. For the curious, `?---? insert` instructs `ex` to find the first match of `---`, and then insert what follows immediately before it. Since we only have one match in that file, which is the bottom of our Role definition, this works nicely. The `.` terminates the `insert` command, after which we do a global search-and-replace for `echo-ro` with `echo-rw`, to clarify the role.

### Verifying RBAC (again)

First, run a `helm upgrade` as before to apply the new role. Then, let's test it out:

    ‚ùØ kubectl exec -it -n echo echo-75897c68fd-nhn64 -- sh
    /app #

Excellent! Not that we have logs in this simple example, but as an exercise, first try getting logs, and if it doesn't work, edit the RBAC as necessary to enable the `echo-user` to read logs.

# Resource Limits and Requests

## Cgroups

First, a quick background.

Docker (and other container runtime) build on Linux Control Groups, or cgroups. Cgroups are a way to split up a system's resources into groups with arbitrary limits, and to control the processes allowed to use each group. This is generally actually a sub-nest in that most Kubernetes nodes are themselves usually run on a VM, which is itself a way of divvying up a system's resources.

If you exec into a container, and you try to get its resources the normal way (`/proc/meminfo` and `/proc/cpuinfo`), you'll get the node's resources, not the container's. This can be confusing if, for instance, you wrote a NodeJS program which calls `totalmem()` to determine available memory. Instead, you need to dive deep into Linux internals.


## Requests vs. Limits

The Kubernetes scheduler will not schedule a pod onto a node that can't fulfill all of its resource requests. If it has 8 GiB of allocatable memory, with 7 GiB used, and you try to request 2 GiB more for a new pod, it will fail to scheduled; the same for CPU. You can, however, set a low request with a high limit, and the pod will be scheduled - you just might not like the performance of your pods later.

Limits, on the other hand, are monitored by the Kubelet. If a container's memory usage exceeds its limit, Linux's OOM killer will kill the container. A container _may_ be allowed to exceed its CPU limit, but this is not a guarantee. Generally, you should expect a container to get throttled if it exceeds its CPU limit. It is for this reason that many state that you should never set CPU limits, only requests, and let spiky workloads be spiky.

## Viewing resources

You can, of course, use `kubectl get pods -n echo $pod_name -o yaml` to get a pod (and therefore container)'s resources, but if you really want to see what Linux has assigned, you need to dig deeper.

### Viewing resources in a container

    # Memory limits are in /sys/fs/cgroup/memory/memory.limit_in_bytes
    # We can use the bash-ism `(())` to do math, converting it to MiB
    # Alternately if you have `bc`, you can use that, as well as `awk`
    ‚ùØ echo $(($(< /sys/fs/cgroup/memory/memory.limit_in_bytes) / 1048576))
    2048

    # Memory requests would be in /sys/fs/cgroup/memory/memory.soft_limit_in_bytes if
    # Kubernetes followed normal Linux memory accounting practices, but it doesn't

    ‚ùØ cat /sys/fs/cgroup/memory/memory.soft_limit_in_bytes
    9223372036854771712


    Wondering what on earth 9223372036854771712 bytes is? Is this a hint?
    ‚ùØ printf "%x\n" $(< /sys/fs/cgroup/memory/memory.soft_limit_in_bytes)
    7ffffffffffff000

    # CPU requests are in /sys/fs/cgroup/cpu/cpu.shares, with a single core/vCPU being equal to 1024
    # This is thus 256 / 1024 == 0.25
    ‚ùØ cat /sys/fs/cgroup/cpu/cpu.shares
    256

    # CPU limits have to be calculated, as it's a combination of quota and period
    ‚ùØ cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us
    150000

    ‚ùØ cat /sys/fs/cgroup/cpu/cpu.cfs_period_us
    100000

    # So, CPU limits are:
    ‚ùØ echo $(($(< /sys/fs/cgroup/cpu/cpu.cfs_quota_us) / $(< /sys/fs/cgroup/cpu/cpu.cfs_period_us)))
    1 # ???

    # Bash doesn't handle floats, as it turns out - the answer is 1.5 vCPUs
    ‚ùØ awk -v quota="$(< /sys/fs/cgroup/cpu/cpu.cfs_quota_us)" \
    -v period="$(< /sys/fs/cgroup/cpu/cpu.cfs_period_us)" \
    '{print quota/period}' <(echo)
    1.5

### Viewing resources on the host

So what if you want to view a given container's resources from the host? More Linux internals, I'm afraid.

This specific example comes from my homelab (so does the above), but once we have requests and limits set for our application, we can circle back and view them on the `minikube` node.

    # I'm going to look for an app called `radarr` that I know is running on this node

    dell01-k3s-worker-01 [~]$ ps -ax | grep radarr
     3028 ?        S      0:00 s6-supervise radarr
     3030 ?        Ssl  1159:40 /app/radarr/bin/Radarr -nobrowser -data=/config
    10484 pts/0    R+     0:00 grep radarr

    # Then, I'll look at its `/proc` filesystem entry
    dell01-k3s-worker-01 [~]$ cat /proc/3030/cgroup
    15:name=openrc:/k3s-service
    14:name=systemd:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    13:rdma:/
    12:pids:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    11:hugetlb:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    10:net_prio:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    9:perf_event:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    8:net_cls:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    7:freezer:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    6:devices:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    5:memory:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    4:blkio:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    3:cpuacct:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    2:cpu:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    1:cpuset:/kubepods/burstable/pod78e3f455-3991-4e0c-a076-07ad534e7a95/2d3023473e0cc6e72b8c5b52007d7e315c6e0b283ad95b86978a315cc3028543
    0::/k3s-service

    # cgroups inherit from their parents, incidentally, so everything here is inheriting
    # from both the `burstable` and `kubepods` cgroups

    # We'll use `awk` to grab what we want from that list, then command substitution
    dell01-k3s-worker-01 [~]$ ls -l /sys/fs/cgroup/memory/$(awk -F: '/memory/ {print $NF}' /proc/3030/cgroup)
    total 0
    -rw-r--r-- 1 root root 0 May 18 15:39 cgroup.clone_children
    --w--w--w- 1 root root 0 May  5 17:50 cgroup.event_control
    -rw-r--r-- 1 root root 0 May 18 15:51 cgroup.procs
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.failcnt
    --w------- 1 root root 0 May 18 15:51 memory.force_empty
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.kmem.failcnt
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.kmem.limit_in_bytes
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.kmem.max_usage_in_bytes
    -r--r--r-- 1 root root 0 May 18 15:51 memory.kmem.slabinfo
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.kmem.tcp.failcnt
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.kmem.tcp.limit_in_bytes
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.kmem.tcp.max_usage_in_bytes
    -r--r--r-- 1 root root 0 May 18 15:39 memory.kmem.tcp.usage_in_bytes
    -r--r--r-- 1 root root 0 May 18 15:39 memory.kmem.usage_in_bytes
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.limit_in_bytes
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.max_usage_in_bytes
    -rw-r--r-- 1 root root 0 May 18 15:51 memory.move_charge_at_immigrate
    -r--r--r-- 1 root root 0 May 18 15:39 memory.numa_stat
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.oom_control
    ---------- 1 root root 0 May 18 15:51 memory.pressure_level
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.soft_limit_in_bytes
    -r--r--r-- 1 root root 0 May 18 15:39 memory.stat
    -rw-r--r-- 1 root root 0 May 18 15:51 memory.swappiness
    -r--r--r-- 1 root root 0 May 18 15:39 memory.usage_in_bytes
    -rw-r--r-- 1 root root 0 May 18 15:39 memory.use_hierarchy
    -rw-r--r-- 1 root root 0 May 18 15:51 notify_on_release
    -rw-r--r-- 1 root root 0 May 18 15:51 tasks

    # Looks familiar, right?

    dell01-k3s-worker-01 [~]$ echo $(($(< /sys/fs/cgroup/memory/$(awk -F: '/memory/ {print $NF}' /proc/3030/cgroup)/memory.limit_in_bytes) / 1048576))
    2048

    # Finding the CPU information from the host is left as an exercise for the reader.

## Setting resource limits and requests

    ‚ùØ cat << EOF >> helm/templates/deployment.yaml
            resources:
              limits:
                cpu: 100m
                memory: 128Mi
              requests:
                cpu: 50m
                memory: 50Mi
    EOF

Now run a `helm upgrade` cycle (make sure you're back to the `minikube` context), then exec back into the pod to examine it.

    /app # echo $(($(< /sys/fs/cgroup/memory/memory.usage_in_bytes) / 1048576))
    sh: arithmetic syntax error

    # As it turns out, the $(< ) command is a bash-ism for `cat`, and this is `sh`, not `bash`

    /app # echo $(($(cat /sys/fs/cgroup/memory/memory.usage_in_bytes) / 1048576))
    37

    # So, our app is using about 37 MiB of memory.

    /app # echo $(($(cat /sys/fs/cgroup/memory/memory.limit_in_bytes) / 1048576))
    128

    And we can see that our 128 MiB limit has been set.

## Exploring limits and requests

Play around (you can use `kubectl edit deployment` to speed things up) with limits and requests, and see how the scheduler and kubelet respond to combinations.

# More to explore

* This application could be put behind a load balancer (you could set up [MetalLB](https://metallb.universe.tf/) locally if you'd like), with additional replicas.
* User entries could be captured and sent to a database stored in a dynamically generated Persistent Volume, with additional routes enabling historical views.
* HPA (Horizontal Pod Autoscaler) could be set up, along with some load testing mechanism, to demonstrate how Kubernetes will scale the application in response to demand.
* KEDA (Kubernetes Event-driven Autoscaling) could be set up to automatically scale on metrics other than CPU or Memory.
